# TransformerEngineINT8ï¼ˆç”»é¥¼ç‰ˆ readmeï¼‰

[](https://www.google.com/search?q=https://github.com/your-username/TransformerEngineINT8)
[](https://www.google.com/search?q=https://pypi.org/project/transformer-engine-int8/)
[](https://opensource.org/licenses/Apache-2.0)
[](https://www.google.com/search?q=https://your-username.github.io/TransformerEngineINT8/)

**TransformerEngineINT8** æ˜¯ä¸€ä¸ªä¸ºPyTorchæ‰“é€ çš„é«˜æ€§èƒ½INT8é‡åŒ–åŠ é€Ÿæ¡†æ¶ï¼Œå…¶è®¾è®¡çµæ„Ÿæºè‡ªNVIDIAçš„[Transformer Engine](https://github.com/NVIDIA/TransformerEngine)ã€‚å®ƒæ—¨åœ¨é€šè¿‡ä¸€ä¸ªæå…¶ç®€æ´çš„APIï¼Œæä¾›æ— ç¼çš„é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQuantization-Aware Training, QATï¼‰ã€‚

## æ¦‚è¿° (Overview)

åœ¨ä¸ä¿®æ”¹ä»»ä½•æ¨¡å‹å®šä¹‰ä»£ç çš„å‰æä¸‹ï¼Œ`TransformerEngineINT8` å…è®¸æ‚¨ä½¿ç”¨ä¸€ä¸ªç®€å•çš„ `autocast` ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œå°†æ¨¡å‹ä¸­çš„è®¡ç®—å¯†é›†å‹éƒ¨åˆ†ï¼ˆå¦‚`Linear`å±‚ï¼‰è‡ªåŠ¨åˆ‡æ¢åˆ°INT8æ¨¡å¼ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨å‡ ä¹ä¸æŸå¤±ç²¾åº¦çš„æƒ…å†µä¸‹ï¼ˆmaybeï¼‰ï¼Œäº«å—åˆ°INT8å¸¦æ¥çš„æ˜¾è‘—çš„å†…å­˜èŠ‚çœå’Œæ½œåœ¨çš„æ€§èƒ½æå‡ã€‚


## è·¯çº¿å›¾ (Roadmap)

æˆ‘ä»¬è‡´åŠ›äºå°†`TransformerEngineINT8`æ‰“é€ æˆä¸€ä¸ªç”Ÿäº§çº§çš„é‡åŒ–åŠ é€Ÿåº“ã€‚

  * [ ] ğŸŸ¡ å®Œæˆ linear çš„ QAT å®éªŒä¸å¯è¡Œæ€§éªŒè¯
  * [ ] âšªï¸ æ ¸å¿ƒAPI (`te_int8_autocast`) ä¸æ¡†æ¶è®¾è®¡
  * [ ] âšªï¸ åŸºäºçº¯PyTorchçš„é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰åŠŸèƒ½å®ç°
  * [ ] âšªï¸ åŸºäºçº¯PyTorchçš„INT8æ¨ç†æ¨¡æ‹ŸåŠŸèƒ½å®ç°
  * [ ] âšªï¸ **é«˜æ€§èƒ½CUDA Kernels**: ä¸º`Linear`å±‚ç­‰å®ç°åŸºäºCUTLASSæˆ–cuBLASçš„INT8 GEMM Kernelï¼Œæä¾›çœŸå®çš„æ€§èƒ½åŠ é€Ÿã€‚
  * [ ] âšªï¸ **`torch.compile` åç«¯é›†æˆ**: å¼€å‘ä¸€ä¸ªè‡ªå®šä¹‰åç«¯ï¼Œå°†æ¡†æ¶çš„ä¼˜åŒ–èƒ½åŠ›ä¸PyTorch 2.xçš„ç¼–è¯‘å™¨æ— ç¼é›†æˆã€‚
  * [ ] âšªï¸ **æ‰©å±•æ¨¡å—æ”¯æŒ**: å¢åŠ å¯¹`nn.Conv2d`ã€`nn.LayerNorm`ç­‰æ›´å¤šæ¨¡å—çš„é‡åŒ–æ”¯æŒã€‚
  * [ ] âšªï¸ **å…¨é¢çš„æ–‡æ¡£ä¸æ•™ç¨‹**: æä¾›è¯¦ç»†çš„APIæ–‡æ¡£ã€ç”¨æˆ·æŒ‡å—å’Œæ€§èƒ½è°ƒä¼˜æŠ€å·§ã€‚



## çªå‡ºç‰¹æ€§ (Key Features)

  * **âš¡ æç®€çš„API**: åªéœ€å°†æ‚¨çš„ä»£ç åŒ…è£¹åœ¨ `te_int8_autocast` ä¸Šä¸‹æ–‡ä¸­ï¼Œå³å¯å¯ç”¨INT8é‡åŒ–ï¼Œæ— éœ€æ‰‹åŠ¨ä¿®æ”¹æ¨¡å‹ã€‚
  * **ğŸš€ é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ (QAT)**: å†…ç½®å¯¹QATçš„å®Œæ•´æ”¯æŒï¼Œé€šè¿‡ç›´é€šä¼°è®¡å™¨ï¼ˆStraight-Through Estimatorï¼‰æŠ€æœ¯ï¼Œåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­æ¨¡æ‹Ÿé‡åŒ–æ•ˆåº”ï¼Œæœ€å¤§ç¨‹åº¦åœ°ä¿æŒæ¨¡å‹ç²¾åº¦ã€‚
  * ** seamlessly é›†æˆPyTorch**: ä½œä¸ºä¸€ä¸ªçº¯ç²¹çš„PyTorchæ‰©å±•ï¼Œä¸ç°æœ‰çš„ç”Ÿæ€ç³»ç»Ÿã€æ¨¡å‹å’Œè®­ç»ƒå¾ªç¯æ— ç¼é›†æˆã€‚
  * **ğŸ”§ æ¨¡å—åŒ–ä¸å¯æ‰©å±•**: æ¸…æ™°çš„æ¶æ„è®¾è®¡ï¼Œå½“å‰ä½¿ç”¨çº¯PyTorchåç«¯è¿›è¡ŒåŠŸèƒ½éªŒè¯ï¼Œå¹¶ä¸ºæœªæ¥çš„é«˜æ€§èƒ½CUDA Kernelé›†æˆå’Œ`torch.compile`åç«¯å¼€å‘é¢„ç•™äº†æ¥å£ã€‚



## æ¶æ„ä¸åŸç† (Architecture & Principles)

`TransformerEngineINT8` çš„æ ¸å¿ƒåœ¨äºå…¶**åŠ¨æ€æ¨¡å—æ›¿æ¢**æœºåˆ¶ã€‚å½“è¿›å…¥`te_int8_autocast`ä¸Šä¸‹æ–‡æ—¶ï¼Œæ¡†æ¶ä¼šï¼š

1.  éå†æ¨¡å‹è®¡ç®—å›¾ï¼ŒæŸ¥æ‰¾æ‰€æœ‰ç›®æ ‡æ¨¡å—ï¼ˆå¦‚ `nn.Linear`ï¼‰ã€‚
2.  å°†å…¶åŠ¨æ€æ›¿æ¢ä¸ºä¸€ä¸ªå†…ç½®çš„ã€å¯æ„ŸçŸ¥é‡åŒ–çš„æ¨¡å— (`QuantizedLinear`)ï¼Œæ‰§è¡ŒQATæˆ–æ¨ç†é€»è¾‘ã€‚
3.  é€€å‡ºä¸Šä¸‹æ–‡æ—¶ï¼Œæ‰€æœ‰æ¨¡å—å°†è‡ªåŠ¨æ¢å¤åŸçŠ¶ï¼Œç¡®ä¿å¯¹åŸå§‹æ¨¡å‹é›¶ä¾µå…¥ã€‚

åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ï¼Œæˆ‘ä»¬é‡‡ç”¨**ç›´é€šä¼°è®¡å™¨ (Straight-Through Estimator, STE)** æ¥è§£å†³é‡åŒ–æ“ä½œï¼ˆå¦‚`round()`ï¼‰ä¸å¯å¯¼çš„é—®é¢˜ï¼Œä»è€Œä¿è¯æ¢¯åº¦èƒ½å¤Ÿé¡ºç•…åœ°å›ä¼ è‡³å…¨ç²¾åº¦çš„â€œå½±å­æƒé‡â€ï¼Œå®ç°çœŸæ­£çš„ç«¯åˆ°ç«¯å¾®è°ƒã€‚



## å®‰è£… (Installation)ï¼ˆå³å°†æ¨å‡ºï¼‰

æ‚¨å¯ä»¥é€šè¿‡pipä»PyPIå®‰è£…ï¼š

```bash
pip install transformer-engine-int8
```

æˆ–è€…ï¼Œä»æºä»£ç å®‰è£…ä»¥è·å–æœ€æ–°ç‰ˆæœ¬ï¼š

```bash
git clone https://github.com/your-username/TransformerEngineINT8.git
cd TransformerEngineINT8
pip install -e .
```

## å¿«é€Ÿä¸Šæ‰‹ (Quick Start)ï¼ˆå³å°†æ¨å‡ºï¼‰

ä½“éªŒ`TransformerEngineINT8`çš„å¼ºå¤§åŠŸèƒ½åªéœ€ä¸¤æ­¥ï¼šé¦–å…ˆè¿›è¡Œé‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆå¾®è°ƒï¼‰ï¼Œç„¶åè¿›è¡Œé‡åŒ–æ¨ç†ã€‚

```python
import torch
import torch.nn as nn
from transformer_engine_int8 import te_int8_autocast

# 1. å®šä¹‰æˆ–åŠ è½½ä¸€ä¸ªæ ‡å‡†çš„PyTorch Transformeræ¨¡å‹
# (è¿™é‡Œæˆ‘ä»¬ç”¨ä¸€ä¸ªç®€å•çš„æ¨¡å‹ä½œä¸ºç¤ºä¾‹)
class SimpleTransformer(nn.Module):
    def __init__(self, d_model=512, nhead=8, num_layers=2):
        super().__init__()
        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, d_model*4, batch_first=True)
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)
        self.output_layer = nn.Linear(d_model, 10) # è¾“å‡ºå±‚

    def forward(self, src):
        memory = self.encoder(src)
        return self.output_layer(memory[:, 0, :]) # å–[CLS] tokenè¾“å‡º

# --- æ­¥éª¤ä¸€ï¼šé‡åŒ–æ„ŸçŸ¥è®­ç»ƒ (QAT) ---

# åŠ è½½é¢„è®­ç»ƒå¥½çš„FP32æ¨¡å‹
model = SimpleTransformer().cuda()
# model.load_state_dict(torch.load('pretrained_fp32.pt'))
optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)
criterion = nn.CrossEntropyLoss()
dummy_data = torch.randn(16, 128, 512).cuda()
dummy_target = torch.randint(0, 10, (16,)).cuda()

print("--- Starting Quantization-Aware Training (QAT) ---")
model.train()
optimizer.zero_grad()
# ä½¿ç”¨ te_int8_autocast å¼€å¯QATæ¨¡å¼
with te_int8_autocast(training=True):
    # åœ¨æ­¤ä¸Šä¸‹æ–‡ä¸­, æ‰€æœ‰å†…éƒ¨çš„nn.Linearéƒ½å°†ä»¥QATæ¨¡å¼è¿è¡Œ
    output = model(dummy_data)
    loss = criterion(output, dummy_target)
    
# æ¢¯åº¦è®¡ç®—å’Œå‚æ•°æ›´æ–°åœ¨ä¸Šä¸‹æ–‡å¤–éƒ¨è¿›è¡Œ
loss.backward()
optimizer.step()
print("QAT step completed. Loss:", loss.item())


# --- æ­¥éª¤äºŒï¼šé‡åŒ–æ¨ç† ---

# åŠ è½½ç»è¿‡QATå¾®è°ƒçš„æ¨¡å‹
# model.load_state_dict(torch.load('qat_tuned.pt'))
model.eval()

print("\n--- Starting Quantized Inference ---")
with torch.no_grad():
    # ä½¿ç”¨åŒä¸€ä¸ªAPIï¼Œä½†å°†trainingè®¾ç½®ä¸ºFalseä»¥è¿›å…¥æ¨ç†æ¨¡å¼
    with te_int8_autocast(training=False):
        # åœ¨æ­¤ä¸Šä¸‹æ–‡ä¸­, nn.Linearå°†ä»¥é«˜æ€§èƒ½INT8æ¨ç†æ¨¡å¼è¿è¡Œ
        quantized_output = model(dummy_data)

print("Inference completed. Output shape:", quantized_output.shape)

```

-----
