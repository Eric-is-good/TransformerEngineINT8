## 实验一：`ax + b`

- **测试文件:** `t2.py`

#### 实验设置
- **矩阵形状**: `4096×4096 @ 4096×4096`
- **Block Size K**: `4096`

#### 实验结果

| 数据类型 | 耗时 | INT8/FP16 速度比 | 绝对误差 | 相对误差 |
| :--- | :--- | :--- | :--- | :--- |
| **FP16** | 9.237 ms | 1.00x | - | - |
| **INT8 (细粒度)** | 11.435 ms | **0.81x** | `0.01204` | **`1.268%`** |

## 实验二：前馈网络 (FFN)

- **测试文件:** `t4.py`

#### 实验设置
- **网络结构**: `FFN(Linear(4096,16384) -> ReLU -> Linear(16384,4096)) + Residual + LayerNorm`
- **输入形状**: `4096×4096`
- **Block Size K**: `128`
- **误差平均次数**: 10

#### 实验结果

| 数据类型 | 端到端性能 | INT8/FP16 速度比 | 平均绝对误差 | 平均相对误差 |
| :--- | :--- | :--- | :--- | :--- |
| **FP16** | 41.707 ms | 1.00x | - | - |
| **INT8** | 926.876 ms | **0.04x** | `0.01467` | **`1.838%`** |

## 结论

在随机初始化的情况下：
1.  对于基础的 `ax + b` 运算，INT8 量化引入的**相对误差约为 1.27%**。
2.  对于一个完整的 FFN 模型，端到端的 INT8 量化**平均相对误差约为 1.84%**。

实验结果表明，在当前测试配置下，量化带来的误差在 2% 左右，希望 2% 的误差可以训练网络 QwQ